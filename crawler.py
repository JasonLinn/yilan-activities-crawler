import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import logging
import time
import random
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_session():
    """å‰µå»ºä¸€å€‹é…ç½®å¥½çš„ requests session"""
    session = requests.Session()
    
    # è¨­å®šé‡è©¦ç­–ç•¥
    retry_strategy = Retry(
        total=3,  # ç¸½é‡è©¦æ¬¡æ•¸
        backoff_factor=1,  # é€€é¿å› å­
        status_forcelist=[429, 500, 502, 503, 504],  # éœ€è¦é‡è©¦çš„HTTPç‹€æ…‹ç¢¼
        allowed_methods=["HEAD", "GET", "OPTIONS"]  # å…è¨±é‡è©¦çš„HTTPæ–¹æ³•
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    # è¨­å®š headers
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    })
    
    return session

def make_request_with_retry(session, url, max_retries=3, base_timeout=10):
    """ç™¼é€è«‹æ±‚ä¸¦è™•ç†é‡è©¦é‚è¼¯"""
    for attempt in range(max_retries):
        try:
            # è¨ˆç®—ç•¶å‰å˜—è©¦çš„è¶…æ™‚æ™‚é–“ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
            timeout = base_timeout * (2 ** attempt)
            
            logger.info(f"å˜—è©¦ç¬¬ {attempt + 1} æ¬¡è«‹æ±‚ {url} (è¶…æ™‚: {timeout}ç§’)")
            
            response = session.get(url, timeout=timeout)
            response.raise_for_status()  # æª¢æŸ¥HTTPéŒ¯èª¤
            return response
            
        except requests.exceptions.Timeout:
            logger.warning(f"ç¬¬ {attempt + 1} æ¬¡è«‹æ±‚è¶…æ™‚")
            if attempt < max_retries - 1:
                wait_time = random.uniform(1, 3) * (attempt + 1)
                logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
            
        except requests.exceptions.ConnectionError as e:
            logger.warning(f"ç¬¬ {attempt + 1} æ¬¡é€£æ¥éŒ¯èª¤: {e}")
            if attempt < max_retries - 1:
                wait_time = random.uniform(2, 5) * (attempt + 1)
                logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
                
        except requests.exceptions.RequestException as e:
            logger.warning(f"ç¬¬ {attempt + 1} æ¬¡è«‹æ±‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            if attempt < max_retries - 1:
                wait_time = random.uniform(1, 3) * (attempt + 1)
                logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’å¾Œé‡è©¦...")
                time.sleep(wait_time)
    
    # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—
    raise requests.exceptions.RequestException(f"ç¶“é {max_retries} æ¬¡é‡è©¦å¾Œä»ç„¡æ³•é€£æ¥åˆ° {url}")

def get_activity_details(session, activity_url):
    """ç²å–æ´»å‹•è©³ç´°è³‡è¨Šï¼ŒåŒ…æ‹¬åœ–ç‰‡"""
    try:
        response = make_request_with_retry(session, activity_url, max_retries=2, base_timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        details = {}
        
        # å°‹æ‰¾æ´»å‹•åœ–ç‰‡
        images = []
        img_tags = soup.find_all('img')
        
        for img in img_tags:
            src = img.get('src', '')
            alt = img.get('alt', '')
            
            # åªæ”¶é›†ä¸Šå‚³çš„æ´»å‹•åœ–ç‰‡
            if 'upload/event/' in src:
                if src.startswith('/'):
                    full_url = f"https://yilanart.ilccb.gov.tw{src}"
                elif not src.startswith('http'):
                    full_url = f"https://yilanart.ilccb.gov.tw/{src}"
                else:
                    full_url = src
                
                images.append({
                    'url': full_url,
                    'alt': alt or 'æ´»å‹•åœ–ç‰‡'
                })
        
        details['images'] = images
        
        # å˜—è©¦ç²å–æ›´è©³ç´°çš„æ´»å‹•æè¿°
        content_div = soup.find('div', class_='content')
        if content_div:
            # å°‹æ‰¾æ´»å‹•æè¿°æ–‡å­—
            text_elements = content_div.find_all(['p', 'div'], string=True)
            description_parts = []
            for elem in text_elements:
                text = elem.get_text(strip=True)
                if len(text) > 20 and 'æ´»å‹•' in text:  # å¯èƒ½æ˜¯æ´»å‹•æè¿°
                    description_parts.append(text)
            
            if description_parts:
                details['description'] = ' '.join(description_parts[:2])  # å–å‰å…©æ®µ
        
        return details
        
    except Exception as e:
        logger.warning(f"ç²å–æ´»å‹•è©³æƒ…å¤±æ•— {activity_url}: {e}")
        return {'images': [], 'description': ''}

def crawl_yilan_activities():
    """çˆ¬å–å®œè˜­ç¸£æ–‡åŒ–å±€æ´»å‹•è³‡è¨Š"""
    url = "https://yilanart.ilccb.gov.tw/index.php?inter=activity"
    
    try:
        logger.info(f"é–‹å§‹çˆ¬å–: {url}")
        
        # ä½¿ç”¨æ–°çš„ session å‰µå»ºå‡½æ•¸
        session = create_session()
        
        # å¿½ç•¥ SSL æ†‘è­‰é©—è­‰å•é¡Œ
        session.verify = False
        
        # ç¦ç”¨ SSL è­¦å‘Š
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        # ä½¿ç”¨æ–°çš„é‡è©¦è«‹æ±‚å‡½æ•¸
        response = make_request_with_retry(session, url, max_retries=3, base_timeout=10)
        
        # è¨­å®šæ­£ç¢ºçš„ç·¨ç¢¼
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        logger.info("æˆåŠŸå–å¾—ç¶²é å…§å®¹")
        
        activities = []
        
        # æ ¹æ“šå¯¦éš›ç¶²é çµæ§‹èª¿æ•´é¸æ“‡å™¨
        # æ‰¾åˆ° content div
        content_div = soup.find('div', class_='content')
        
        if content_div:
            # å°‹æ‰¾æ´»å‹•é€£çµï¼Œæ’é™¤å°èˆªé€£çµ
            activity_links = content_div.find_all('a', href=True)
            
            for link in activity_links:
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # åªè™•ç†æ´»å‹•è©³æƒ…é€£çµï¼ˆåŒ…å« did= åƒæ•¸çš„é€£çµï¼‰
                if 'did=' in href and text:
                    try:
                        activity = {}
                        
                        # è§£ææ´»å‹•æ–‡å­—ï¼Œæ ¼å¼é€šå¸¸æ˜¯ï¼šåˆ†é¡æ—¥æœŸæ¨™é¡Œåœ°é»
                        import re
                        
                        # æå–æ´»å‹•æ¨™é¡Œï¼ˆæœ€å¾Œçš„æ–‡å­—éƒ¨åˆ†ï¼‰
                        # åˆ†å‰²æ–‡å­—ï¼Œå˜—è©¦æ‰¾åˆ°æ¨¡å¼
                        parts = text.split('å…ç¥¨å…¥å ´') if 'å…ç¥¨å…¥å ´' in text else text.split('ç·šä¸Šè³¼ç¥¨') if 'ç·šä¸Šè³¼ç¥¨' in text else [text]
                        
                        if len(parts) >= 2:
                            # æœ‰ç¥¨åƒ¹è³‡è¨Š
                            activity['title'] = parts[0].strip()
                            activity['price'] = 'å…ç¥¨å…¥å ´' if 'å…ç¥¨å…¥å ´' in text else 'ç·šä¸Šè³¼ç¥¨' if 'ç·šä¸Šè³¼ç¥¨' in text else 'æœªçŸ¥'
                            activity['location'] = parts[1].strip() if len(parts) > 1 else ''
                        else:
                            # æ²’æœ‰æ˜ç¢ºçš„ç¥¨åƒ¹è³‡è¨Šï¼Œå˜—è©¦å…¶ä»–åˆ†å‰²æ–¹å¼
                            activity['title'] = text
                            activity['price'] = 'æœªçŸ¥'
                            activity['location'] = ''
                        
                        # å˜—è©¦å¾æ¨™é¡Œä¸­æå–æ—¥æœŸ
                        date_pattern = r'(\d{4})(\d{2})\.(\d{2})\.([A-Za-z]{3})'
                        date_match = re.search(date_pattern, text)
                        if date_match:
                            year, month, day, day_name = date_match.groups()
                            activity['date'] = f"{year}-{month}-{day} ({day_name})"
                            # æ¸…ç†æ¨™é¡Œï¼Œç§»é™¤æ—¥æœŸéƒ¨åˆ†
                            activity['title'] = re.sub(date_pattern, '', activity['title']).strip()
                        
                        # æå–æ´»å‹•é¡å‹ï¼ˆå¦‚æœæ–‡å­—é–‹é ­æœ‰åˆ†é¡ï¼‰
                        category_match = re.match(r'^(å±•è¦½|è¡¨æ¼”|ç ”ç¿’|æ•…äº‹|æ´»å‹•)', text)
                        if category_match:
                            activity['category'] = category_match.group(1)
                            # å¾æ¨™é¡Œä¸­ç§»é™¤åˆ†é¡
                            activity['title'] = re.sub(r'^(å±•è¦½|è¡¨æ¼”|ç ”ç¿’|æ•…äº‹|æ´»å‹•)', '', activity['title']).strip()
                        
                        # å»ºæ§‹å®Œæ•´çš„æ´»å‹•é€£çµ
                        if href.startswith('/'):
                            activity['url'] = f"https://yilanart.ilccb.gov.tw{href}"
                        elif not href.startswith('http'):
                            activity['url'] = f"https://yilanart.ilccb.gov.tw/{href}"
                        else:
                            activity['url'] = href
                        
                        # ç²å–æ´»å‹•è©³ç´°è³‡è¨Šï¼ˆåŒ…æ‹¬åœ–ç‰‡ï¼‰
                        logger.info(f"ç²å–æ´»å‹•è©³æƒ…: {activity.get('title', 'æœªçŸ¥æ´»å‹•')}")
                        details = get_activity_details(session, activity['url'])
                        activity.update(details)
                        
                        # æ·»åŠ çˆ¬å–æ™‚é–“
                        activity['crawl_time'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        
                        # æ¸…ç†æ¨™é¡Œä¸­çš„å¤šé¤˜å…§å®¹
                        if activity.get('title'):
                            title = activity['title']
                            # ç§»é™¤å¯èƒ½çš„åœ°é»è³‡è¨Šï¼ˆå¦‚æœåœ¨æ¨™é¡Œæœ«å°¾ï¼‰
                            locations = ['å®œè˜­æ–‡å­¸é¤¨', 'å®œè˜­ç¾è¡“é¤¨', 'ä¸­èˆˆæ–‡åŒ–å‰µæ„åœ’å€', 'é ­åŸè¦ªå­é¤¨', 
                                       'å®œè˜­æ¼”è—å»³', 'ç¾…æ±æ–‡åŒ–å·¥å ´', 'ç¾…æ±é®åœ–ä»æ„›é¤¨', 'åŒ–é¾ä¸€æ‘', 'å…¶ä»–åœ°é»']
                            for loc in locations:
                                if title.endswith(loc):
                                    title = title[:-len(loc)].strip()
                                    if not activity['location']:
                                        activity['location'] = loc
                            activity['title'] = title
                        
                        # åªæœ‰æ¨™é¡Œä¸ç‚ºç©ºæ‰åŠ å…¥
                        if activity.get('title') and len(activity['title']) > 3:
                            activities.append(activity)
                            
                    except Exception as e:
                        logger.warning(f"è§£æå–®å€‹æ´»å‹•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                        continue
        else:
            logger.warning("æœªæ‰¾åˆ°æ´»å‹•å…§å®¹å€åŸŸ")
        
        # å„²å­˜çµæœ
        output_dir = 'data'
        os.makedirs(output_dir, exist_ok=True)
        
        # å„²å­˜ç‚º JSON
        json_file = f'{output_dir}/yilan_activities_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(activities, f, ensure_ascii=False, indent=2)
        
        # å„²å­˜æœ€æ–°è³‡æ–™ (è¦†è“‹å¼)
        latest_file = f'{output_dir}/latest_activities.json'
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump({
                'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'total_count': len(activities),
                'activities': activities
            }, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æˆåŠŸçˆ¬å– {len(activities)} ç­†æ´»å‹•è³‡æ–™")
        logger.info(f"è³‡æ–™å·²å„²å­˜è‡³: {json_file}")
        
        return activities
        
    except requests.RequestException as e:
        logger.error(f"ç¶²è·¯è«‹æ±‚éŒ¯èª¤: {e}")
        raise
    except Exception as e:
        logger.error(f"çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise

def generate_readme():
    """ç”Ÿæˆ README æ–‡ä»¶é¡¯ç¤ºæœ€æ–°è³‡æ–™"""
    try:
        with open('data/latest_activities.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        readme_content = f"""# å®œè˜­ç¸£æ–‡åŒ–å±€æ´»å‹•è³‡è¨Š

## ğŸ“Š æœ€æ–°æ›´æ–°è³‡è¨Š
- **æ›´æ–°æ™‚é–“**: {data['update_time']}
- **æ´»å‹•æ•¸é‡**: {data['total_count']} ç­†

## ğŸ­ è¿‘æœŸæ´»å‹•

"""
        
        for i, activity in enumerate(data['activities'][:10], 1):  # åªé¡¯ç¤ºå‰10ç­†
            readme_content += f"### {i}. {activity.get('title', 'ç„¡æ¨™é¡Œ')}\n"
            if activity.get('date'):
                readme_content += f"- **æ™‚é–“**: {activity['date']}\n"
            if activity.get('location'):
                readme_content += f"- **åœ°é»**: {activity['location']}\n"
            if activity.get('category'):
                readme_content += f"- **é¡å‹**: {activity['category']}\n"
            if activity.get('price'):
                readme_content += f"- **ç¥¨åƒ¹**: {activity['price']}\n"
            if activity.get('url'):
                readme_content += f"- **é€£çµ**: [{activity['url']}]({activity['url']})\n"
            
            # æ·»åŠ åœ–ç‰‡
            if activity.get('images') and len(activity['images']) > 0:
                readme_content += f"- **åœ–ç‰‡**:\n"
                for img in activity['images'][:2]:  # æœ€å¤šé¡¯ç¤º2å¼µåœ–ç‰‡
                    readme_content += f"  - ![{img.get('alt', 'æ´»å‹•åœ–ç‰‡')}]({img['url']})\n"
            
            readme_content += "\n"
        
        readme_content += f"""
## ğŸ“ è³‡æ–™æª”æ¡ˆ
- [æœ€æ–°æ´»å‹•è³‡æ–™ (JSON)](./data/latest_activities.json)
- [æ­·å²è³‡æ–™ç›®éŒ„](./data/)

## ğŸ¤– è‡ªå‹•åŒ–èªªæ˜
æ­¤å°ˆæ¡ˆä½¿ç”¨ GitHub Actions æ¯å¤©è‡ªå‹•çˆ¬å–2æ¬¡è³‡æ–™ (09:00 å’Œ 21:00 UTC+8)

---
*æœ€å¾Œæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        with open('README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        logger.info("README.md å·²æ›´æ–°")
        
    except Exception as e:
        logger.warning(f"ç”Ÿæˆ README æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def create_fallback_data():
    """å‰µå»ºå‚™ç”¨è³‡æ–™æª”æ¡ˆï¼Œç•¶çˆ¬å–å¤±æ•—æ™‚ä½¿ç”¨"""
    fallback_data = {
        'update_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': 0,
        'activities': [],
        'status': 'failed',
        'message': 'ç„¡æ³•é€£æ¥åˆ°å®œè˜­æ–‡åŒ–å±€ç¶²ç«™ï¼Œè«‹ç¨å¾Œå†è©¦'
    }
    
    output_dir = 'data'
    os.makedirs(output_dir, exist_ok=True)
    
    # åªæœ‰åœ¨æ²’æœ‰ä»»ä½•è³‡æ–™æ™‚æ‰å‰µå»ºå‚™ç”¨æª”æ¡ˆ
    latest_file = f'{output_dir}/latest_activities.json'
    if not os.path.exists(latest_file):
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(fallback_data, f, ensure_ascii=False, indent=2)
        logger.info("å·²å‰µå»ºå‚™ç”¨è³‡æ–™æª”æ¡ˆ")
    
    return fallback_data

if __name__ == "__main__":
    success = False
    activities = []
    
    try:
        activities = crawl_yilan_activities()
        generate_readme()
        success = True
        
        print(f"âœ… çˆ¬å–å®Œæˆï¼Œå…± {len(activities)} ç­†æ´»å‹•è³‡æ–™")
        
        # é¡¯ç¤ºå‰3ç­†è³‡æ–™ä½œç‚ºé è¦½
        for i, activity in enumerate(activities[:3], 1):
            print(f"\n{i}. {activity.get('title', 'ç„¡æ¨™é¡Œ')}")
            if activity.get('date'):
                print(f"   æ™‚é–“: {activity['date']}")
            if activity.get('location'):
                print(f"   åœ°é»: {activity['location']}")
                
    except requests.exceptions.RequestException as e:
        logger.error(f"ç¶²è·¯é€£ç·šå•é¡Œ: {e}")
        print(f"âš ï¸ ç¶²è·¯é€£ç·šå¤±æ•—ï¼Œé€™å¯èƒ½æ˜¯æš«æ™‚æ€§å•é¡Œ")
        print("å»ºè­°ç¨å¾Œå†è©¦ï¼Œæˆ–æª¢æŸ¥ç¶²ç«™æ˜¯å¦æ­£å¸¸é‹ä½œ")
        
        # å‰µå»ºå‚™ç”¨è³‡æ–™
        fallback_data = create_fallback_data()
        
        # GitHub Actions ç’°å¢ƒä¸‹ä¸è¦ç›´æ¥å¤±æ•—ï¼Œè€Œæ˜¯æä¾›æœ‰ç”¨çš„è³‡è¨Š
        if os.getenv('GITHUB_ACTIONS'):
            print("ğŸ”„ é€™æ˜¯ GitHub Actions ç’°å¢ƒï¼Œå°‡ç¹¼çºŒåŸ·è¡Œè€Œä¸ä¸­æ–·å·¥ä½œæµç¨‹")
            print(f"ğŸ“ ä¸‹æ¬¡æ’ç¨‹åŸ·è¡Œæ™‚é–“æœƒè‡ªå‹•é‡è©¦")
            exit(0)  # ä¸è®“ GitHub Actions å¤±æ•—
        else:
            exit(1)  # æœ¬åœ°åŸ·è¡Œæ™‚é¡¯ç¤ºéŒ¯èª¤
            
    except Exception as e:
        logger.error(f"æ„å¤–éŒ¯èª¤: {e}")
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        
        # å‰µå»ºå‚™ç”¨è³‡æ–™
        fallback_data = create_fallback_data()
        
        # GitHub Actions ç’°å¢ƒä¸‹æä¾›æ›´å¤šè³‡è¨Š
        if os.getenv('GITHUB_ACTIONS'):
            print("ğŸ”„ é€™æ˜¯ GitHub Actions ç’°å¢ƒ")
            print(f"ğŸ“ éŒ¯èª¤è©³æƒ…å·²è¨˜éŒ„ï¼Œä¸‹æ¬¡æ’ç¨‹åŸ·è¡Œæ™‚æœƒè‡ªå‹•é‡è©¦")
            exit(0)  # ä¸è®“ GitHub Actions å¤±æ•—
        else:
            exit(1)  # æœ¬åœ°åŸ·è¡Œæ™‚é¡¯ç¤ºéŒ¯èª¤